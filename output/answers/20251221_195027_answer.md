# Question

What is the main contribution?

# Answer

Based on the provided evidence, the main contribution is the proposal and validation of **DeepSeek-OCR**, a vision-language model (VLM) designed as a proof-of-concept for **"contexts optical compression"** (deepseek_ocr_paper p.1, p.3, p.19).

The specific contributions include:

*   **Vision-Text Compression Paradigm:** The authors propose using visual modality to compress textual information, addressing the computational challenges LLMs face with long contexts. They demonstrate that a single image can represent rich information using substantially fewer tokens than digital text (deepseek_ocr_paper p.3).
*   **Quantitative Feasibility:** The work provides a comprehensive analysis showing that the model can decode text with **97% precision** even when the number of text tokens is 10 times that of the vision tokens (a compression ratio of <10×), and maintains ~60% accuracy at a 20× compression ratio (deepseek_ocr_paper p.1, p.19).
*   **Architecture:** The introduction of a specific architecture comprising **DeepEncoder** (for visual perception and knowledge feature extraction) and a **DeepSeek3B-MoE** decoder (deepseek_ocr_paper p.1, p.5).
*   **Practical Performance:** DeepSeek-OCR demonstrates superior efficiency on the **OmniDocBench** benchmark, outperforming models like GOT-OCR2.0 and MinerU2.0 while using significantly fewer vision tokens (e.g., using 100 tokens compared to 256 or 6000+) (deepseek_ocr_paper p.1).

---

# Retrieved Pages (for debugging)

- Page 3: memory_id=
- Page 19: memory_id=
- Page 1: memory_id=
- Page 12: memory_id=
- Page 5: memory_id=
- Page 2: memory_id=
- Page 14: memory_id=
- Page 13: memory_id=
