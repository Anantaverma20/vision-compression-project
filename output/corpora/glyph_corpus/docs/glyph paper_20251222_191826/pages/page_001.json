{
  "page_number": 1,
  "markdown": "```json\n{\n  \"page_number\": 1,\n  \"markdown\": \"# Glyph: Scaling Context Windows via Visual-Text Compression\\n\\n**Jiale Cheng¹²*, Yusen Liu²*, Xinyu Zhang²*, Yulin Fei²*, Wenyi Hong²³**\\n**Ruiliang Lyu², Weihan Wang², Zhe Su², Xiaotao Gu², Xiao Liu²³, Yushi Bai²³**\\n**Jie Tang³, Hongning Wang¹, Minlie Huang¹†**\\n\\n¹The Conversational Artificial Intelligence (CoAI) Group, Tsinghua University\\n²Zhipu AI\\n³The Knowledge Engineering Group (KEG), Tsinghua University\\n\\n`chengjl23@mails.tsinghua.edu.cn`, `aihuang@tsinghua.edu.cn`\\n\\n## Abstract\\n\\nLarge language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling context windows to the million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. In this work, we take a different perspective—visual context scaling—to tackle this challenge. Instead of extending token-based sequences, we propose Glyph, a framework that renders long texts into images and processes them with vision–language models (VLMs). This approach substantially compresses textual input while preserving semantic information, and we further design an LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression. Through extensive experiments, we demonstrate that our method achieves 3–4× token compression while maintaining accuracy comparable to leading LLMs such as Qwen3-8B on various long-context benchmarks. This compression also leads to around 4× faster prefilling and decoding, and approximately 2× faster SFT training. Furthermore, under extreme compression, a 128K-context VLM could scale to handle 1M-token-level text tasks. In addition, the rendered text data benefits real-world multimodal tasks, such as document understanding. Our code and model are released at https://github.com/thu-coai/Glyph.\\n\\n---\\n\\n**Figure 1:** (Upper) Comparison of two paradigms for long-context tasks: conventional approaches directly feeding plain text into LLMs, and the proposed VLM-based paradigm, Glyph, which renders text as compact images to achieve substantial input-token compression. (Lower) Glyph attains competitive performance on LongBench and MRCR, while offering significant compression and inference speedup over its text backbone model on 128K-token inputs.\\n\\n*[Visual Description of Figure 1 Upper: Diagram showing 'Plain Text' processing (~240K tokens) vs 'Rendering' processing (~80K tokens, 3x Compression) using Images and VLM.]*\\n*[Visual Description of Figure 1 Lower: Bar charts comparing Accuracy (LongBench, MRCR) between Qwen3-8B, GLM-4-9B-Chat-1M, Qwen2.5-7B-Instruct-1M, and Glyph. Another chart shows Compression / Speedup Ratio for Glyph vs Text Backbone Model, highlighting 3.2x Compression, 4.8x Prefilling, and 4.4x Decoding Throughput.]*\\n\\n---\\n\\n## 1 Introduction\\n\\nRecent advances in large language models (LLMs) have enabled remarkable progress across a wide spectrum of real-world tasks (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023; GLM et al., 2024; Yang et al., 2025). As LLMs become increasingly capable, the demand for long-context modeling has grown critical, especially for applications such as document understanding, code analysis, and multi-hop reasoning (Bai et al., 2024; Comanici et al., 2025). However, scaling context windows to hundreds of thousands or even millions of tokens poses prohibitive training and inference costs in both computation and memory, severely limiting the practicality of such models in real-world applications.\\n\\nRecent work has explored",
  "entities": [],
  "summary": ""
}