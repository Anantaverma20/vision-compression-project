{
  "page_number": 2,
  "markdown": "```json\n{\n  \"page_number\": 2,\n  \"markdown\": \"accuracy when extrapolated to much longer sequences (Wu et al., 2024). Another line focuses on modifying the attention mechanism, e.g., sparse or linear attention (Huang et al., 2023; Yang et al., 2024; Peng et al., 2025; Chen et al., 2025a), which reduces the quadratic complexity of self-attention and improves per-token efficiency. Yet, as context length grows to hundreds of thousands of tokens, the overall overhead remains substantial, since the number of tokens is unchanged. Retrieval-augmented approaches (Laban et al., 2024; Yu et al., 2025a) instead shorten the input length through external retrieval, but they risk missing important information and could introduce additional latency.\\n\\nDistinct from the aforementioned approaches, we propose Glyph, a new paradigm that scales context length by rendering plain text into compact images and leveraging vision-language models (VLMs) to process the rendered inputs. In this way, the VLM operates directly on the glyphs of the text—treating each visual token as a compact carrier of multiple textual tokens—thereby increasing the information density without sacrificing semantic fidelity. This glyph-based visual representation allows a fixed-context VLM to process substantially longer texts than a text-only LLM with the same context length, thereby enabling long-context understanding without expanding the context window or relying on external retrieval mechanisms. For example, consider the novel “Jane Eyre” (≈240K text tokens). A conventional 128K–context LLM cannot accommodate the entire book, and truncation easily leads to wrong answers for questions requiring global coverage, such as “Who supports Jane when she is in distress after leaving Thornfield?” In contrast, Glyph renders the book into compact images (e.g. ≈80K visual tokens), enabling a 128K–context VLM to process the full novel and answer such questions reliably.\\n\\nSpecifically, Glyph consists of three main stages, namely, continual pre-training, LLM-driven rendering search, and post-training. In the continual pre-training stage, we render large-scale long-context text into diverse visual forms, enabling the VLM to transfer its long-context capability from text tokens to visual tokens. Since the text-to-image conversion directly determines the trade-off between context compression and model performance, devising an optimal configuration of the conversion is crucial for downstream performance. To this end, we design an LLM-driven genetic search to automatically explore rendering parameters (e.g., font size, layout, resolution) to maximize compression while preserving long-context ability. The resulting configuration is then applied in the post-training stage, where we perform supervised fine-tuning and reinforcement learning to further improve the model’s performance on visualized input. An auxiliary OCR task is applied to enhance the model’s ability to recognize textual content within images, thereby better aligning its visual and textual representations, yielding the final Glyph model.\\n\\nWe conduct extensive experiments to evaluate the performance of Glyph. Results demonstrate that Glyph achieves 3–4× token compression of long sequences while preserving accuracy comparable to state-of-the-art LLMs such as Qwen3-8B. This compression not only extends the effective context length but also improves both training and inference efficiency, yielding up to 4.8× faster pre-filling, 4.4× faster decoding, as well as about 2× faster SFT training. Moreover, we find that incorporating rendered text data effectively enhances performance on real-world multimodal long-context tasks, such as document understanding.\\n\\nOur contributions can be summarized as follows:\\n\\n*   We introduce a novel framework,",
  "entities": [],
  "summary": ""
}