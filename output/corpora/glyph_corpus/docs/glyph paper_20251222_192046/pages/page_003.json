{
  "page_number": 3,
  "markdown": "```json\n{\n  \"page_number\": 3,\n  \"markdown\": \"![Figure 2: Glyph consists of three main stages: continual pre-training on rendered long-text data, LLM-driven genetic search for optimal rendering configurations, and post-training with SFT, RL. Together, these stages enable efficient long-context modeling with visual-text compression.](figure_2)\\n\\n**Figure 2:** Glyph consists of three main stages: continual pre-training on rendered long-text data, LLM-driven genetic search for optimal rendering configurations, and post-training with SFT, RL. Together, these stages enable efficient long-context modeling with visual-text compression.\\n\\n2024; Peng et al., 2025; Chen et al., 2025a), positional interpolation and extrapolation (Su et al., 2021; Press et al., 2021; Sun et al., 2022; Peng et al., 2023), and content-aware encodings (Chen et al., 2025b; Zhu et al., 2024). On the training side, LongAlign (Zhang et al., 2024) had built instruction datasets and loss-weighting strategies for sequences up to 100k tokens, while LongLoRA (Chen et al., 2024) had combined shifted sparse attention with parameter-efficient fine-tuning. LongRecipe (Wang et al., 2024b) had improved efficiency by integrating token analysis, index transformation, and optimization, scaling open-source models from 8k to 128k. ProLong (Liu et al., 2024b) had taken a data-centric view, selecting samples with long-range dependencies. In contrast, our method compresses text into visual tokens, which can be combined with existing techniques to reduce cost and extend context length.\\n\\n## 2.2 Multimodal Large Language Model\\n\\nMultimodal large language models (MLLMs) extend traditional LLMs to process and reason over text and visual inputs jointly. Early studies primarily focus on architectural design and effectively leveraging powerful language backbones, as exemplified by PALI (Chen et al., 2022), LLaVA (Liu et al., 2023), and CogVLM (Wang et al., 2024a). Subsequent work further enhances these models through improvements in both LLM backbones and large-scale vision-language pretraining (Hong et al., 2024a; Bai et al., 2025), while also expanding to additional modalities such as video and audio (Hurst et al., 2024). Notably, MLLMs demonstrate strong capabilities in image perception and optical character recognition (OCR) (Hong et al., 2024b; Liu et al., 2024a), where multiple characters or words can be represented by a single visual token, highlighting the potential for effective context compression.\\n\\n# 3 Method\\n\\nWe present Glyph, a novel paradigm for scaling long-context text understanding through visual compression. Unlike conventional long-context LLMs that extend token-based context windows, Glyph transforms ultra-long textual inputs into compact visual images and processes them with a visionâ€“language model. This fundamentally different modeling method bypasses the prohibitive memory and computation costs of million-token sequences while preserving textual semantics. Furthermore, we introduce an LLM-driven genetic search to automatically discover optimal rendering configurations, ensuring the best trade-",
  "entities": [],
  "summary": ""
}