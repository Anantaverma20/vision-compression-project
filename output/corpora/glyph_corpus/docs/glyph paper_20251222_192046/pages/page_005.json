{
  "page_number": 5,
  "markdown": "```json\n{\n  \"page_number\": 5,\n  \"markdown\": \"produces a model capable of understanding rendered text and handling long contexts, referred to as Glyph-Base.\\n\\n### 3.4 LLM-Driven Rendering Search\\n\\nAlthough diverse rendering improves generalization, downstream tasks often require a specific trade-off between compression and visual clarity for the VLM. We therefore perform an LLM-driven genetic search after continual pre-training to automatically identify the optimal rendering configuration $\\\\theta^*$ used in the post-training stage.\\n\\n**Genetic Algorithm.** Starting from an initial population of candidate configurations $\\\\{\\\\theta_k\\\\}$ sampled from pre-training configurations, we iteratively perform the following steps:\\n\\n1.  **Rendering Data:** render the validation set using each configuration $\\\\theta_k$ to obtain visual inputs.\\n2.  **Evaluation on Validation Set:** perform model inference on the rendered data, measure task accuracy and compression ratio, and update the results.\\n3.  **LLM Analysis & Critique:** use an LLM to suggest promising mutations and crossovers based on the current population and validation results.\\n4.  **Search History:** record all configurations and their performance; rank and sample promising candidates for the next iteration.\\n\\nThis process continues until the population converges, i.e., when no further improvement is observed in validation accuracy or compression over a pre-defined number of generations. The resulting configuration $\\\\theta^*$ is then adopted for post-training.\\n\\n### 3.5 Post-Training\\n\\nWith the optimal rendering configuration $\\\\theta^*$ fixed, we further improve Glyph-Base through two complementary optimization stages—*supervised fine-tuning* and *reinforcement learning*—supplemented by an *auxiliary OCR alignment* task. Together, these components jointly enhance the model's ability to reason over visually compressed inputs and to recognize textual details.\\n\\n**Supervised Fine-Tuning.** To endow the model with robust comprehension under visual inputs, we curate a high-quality text SFT corpus and render its long-context inputs using the optimal configuration. Each response adopts a thinking-style format, in which each example contains explicit reasoning traces (e.g., ``<think>...</think>``). This encourages the model to perform step-by-step reasoning when reading massive token contexts.\\n\\nFormally, the loss function can be written as\\n\\n$$\\n\\\\mathcal{L}_{\\\\text{SFT}} = -\\\\mathbb{E}_{(\\\\mathcal{I}, \\\\mathcal{V}, \\\\mathcal{R})} \\\\sum_t \\\\log P_\\\\phi(r_t \\\\mid \\\\mathcal{I}, \\\\mathcal{V}, r_{<t}), \\\\tag{2}\\n$$\\n\\",
  "entities": [],
  "summary": ""
}