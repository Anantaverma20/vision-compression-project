{
  "page_number": 4,
  "markdown": "```json\n{\n  \"page_number\": 4,\n  \"markdown\": \"achieve both high accuracy and significant gains in token compression, computational efficiency, and memory usage.\\n\\n## 3.2 Task Definition\\n\\n**Task Formulation.** We formalize the standard long-context instruction following task as a triple $(\\\\mathcal{I}, \\\\mathcal{C}, \\\\mathcal{R})$, where $\\\\mathcal{I}$ is a concise user instruction specifying the core goal, $\\\\mathcal{C} = \\\\{c_1, \\\\dots, c_T\\\\}$ is an ultra-long textual context, and $\\\\mathcal{R}$ is the target response. The conventional learning objective is to maximize\\n\\n$$ P(\\\\mathcal{R} \\\\mid \\\\mathcal{I}, \\\\mathcal{C}), $$\\n\\ni.e., to generate an accurate response conditioned on both the instruction and the long textual context.\\n\\nScaling this token-based formulation to million-token contexts, however, imposes prohibitive memory and computation costs. To overcome these limitations, we reformulate the input representation through *visual compression*. Instead of directly feeding $\\\\mathcal{C}$ as text tokens, we render it into a sequence of visual pages $\\\\mathcal{V} = \\\\{v_1, \\\\dots, v_n\\\\}$, each containing the glyphs of multiple text segments. This allows the model to reason over a compressed but semantically equivalent input:\\n\\n$$ P(\\\\mathcal{R} \\\\mid \\\\mathcal{I}, \\\\mathcal{V}). $$\\n\\nEach training instance is thus represented as $(\\\\mathcal{I}, \\\\mathcal{V}, \\\\mathcal{R})$.\\n\\n**Rendering Pipeline.** The rendering pipeline parameterizes how text is visualized before being fed into the model. Each rendering is specified by a configuration vector:\\n\\n$$ \\\\boldsymbol{\\\\theta} = (\\\\texttt{dpi}, \\\\texttt{page\\\\_size}, \\\\texttt{font\\\\_family}, \\\\texttt{font\\\\_size}, \\\\\\\\\\n\\\\texttt{line\\\\_height}, \\\\texttt{alignment}, \\\\texttt{indent}, \\\\texttt{spacing}, \\\\\\\\\\n\\\\texttt{h\\\\_scale}, \\\\texttt{colors}, \\\\texttt{borders}, \\\\dots), $$\\n\\nwhich controls typography, layout, and visual style of the rendered pages. Given the context $\\\\mathcal{C}$ and configuration $\\\\boldsymbol{\\\\theta}$, the pipeline produces a sequence of images that serve as the VLM's long-context input.\\n\\nTo quantify the degree of compression, we define the compression ratio:\\n\\n$$ \\\\rho(\\\\boldsymbol{\\\\theta}) = \\\\frac{|\\\\mathcal{C}|}{\\\\sum_{i=1}^n \\\\tau(v_i)}, $$\\n\\nwhere $\\\\tau(v_i)$ denotes the number of visual tokens consumed by page $v_i$. A higher $\\\\rho$ indicates that each visual token encodes more textual information, thus achieving stronger compression.\\n\\nIn practice, $\\\\boldsymbol{\\\\theta}$ determines both information density (through font size, dpi) and visual clarity (through layout and spacing). By varying $\\\\boldsymbol{\\\\theta}$, we can continuously adjust the balance between compression and readability for the VLM.\\n\\n## 3.3 Continual Pre-Training\\n\\nThe purpose of continual pre-training is to transfer long-context comprehension from the textual to the visual modality. This stage exposes the VLM to a wide range of rendering styles and tasks so that it can align the semantics between rendered images and their corresponding texts.\\n\\n**Data Construction.** To enhance model robustness, better aligning long-text",
  "entities": [],
  "summary": ""
}