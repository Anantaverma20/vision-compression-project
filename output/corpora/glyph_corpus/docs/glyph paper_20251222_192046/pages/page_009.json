{
  "page_number": 9,
  "markdown": "```json\n{\n  \"page_number\": 9,\n  \"markdown\": \"model on MRCR with sequence lengths extended from 128k to 1024k. The results (Table 7) show that Glyph successfully demonstrates the potential for 8× effective context expansion, achieving performance on par with GLM-4-9B-Chat-1M and Qwen2.5-1M. This experiment highlights that our method can indeed be pushed to more extreme compression regimes while retaining performance, suggesting substantial headroom for extending usable context far beyond current limits, like a model that can deal with 4M, even 8M context tokens.\\n\\n## 5 Conclusion\\n\\nIn this work, we present Glyph, an efficient long-context modeling framework that renders long texts into compact images and processes them with vision-language models. With continual pre-training, an LLM-driven genetic rendering search and targeted post-training, Glyph achieves 3–4× context compression while maintaining competitive performance with similar size leading LLMs such as Qwen3-8B. Extensive experiments further demonstrate substantial gains in inference speed and memory efficiency, and show that our method demonstrates cross-modal benefits, enhancing multimodal long-context tasks like document understanding. Our findings demonstrate that enhancing token information density constitutes a promising new paradigm for scaling long-context LLMs, orthogonal to existing attention-based approaches, and there remains great room for further exploration in depth.\\n\\n## Limitations and Future Work\\n\\nDespite the effectiveness of Glyph and its strong potential for broader applications, we want to discuss several limitations of the current work that are worth further exploration.\\n\\n**Sensitivity to rendering parameters.** Our method relies on rendering textual inputs into images before processing. We find that performance can be noticeably affected by rendering configurations such as resolution, font, and spacing. Although our search procedure allows us to identify a configuration that performs well on downstream tasks, how to make the model more robust across various rendering settings remains an open problem.\\n\\n**OCR-related challenges.** As discussed in the Ruler benchmark, UUID recognition remains particularly challenging for current VLMs, and even the strongest models (e.g., Gemini-2.5-Pro) often fail to reproduce them correctly. Such rare alphanumeric sequences frequently result in disordered or misclassified characters, which may stem from their distributional sparsity in training data or from architectural limitations of visual encoders. While these cases have little impact on most tasks, improving OCR fidelity could push the upper bound of our approach.\\n\\n**Task diversity.** The benchmarks in this work mainly focus on long-context understanding. While these tasks provide a strong proof of concept, they do not fully capture the diversity of real-world applications, such as agentic or reasoning-heavy tasks. We also observe that, compared with textual models, the visual-text model tends to generalize less effectively across tasks. Extending the scope of evaluation and training to a wider range of tasks will help better assess and improve the robustness and generality of our approach.\\n\\n**Future directions.** Building upon the current study, several directions could further advance the proposed visual–text compression paradigm. First, rather than using a fixed rendering strategy, one promising avenue is to train adaptive rendering models that condition on the task type or user query, producing tailored visualizations that balance compression and performance. Second, enhancing the visual encoder’s capability for fine-grained text recognition and alignment with language representations could improve robustness and transferability across tasks. Third, improving the alignment between visual–text and purely textual models, for instance, through knowledge distillation or cross-modal supervision, could narrow the performance gap in generalization. Fourth, our approach could be extended to broader applications, such as agent memory systems capable of managing long-term conversations or agentic contexts, and tasks that can leverage structured visual",
  "entities": [],
  "summary": ""
}