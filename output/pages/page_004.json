{
  "page_number": 4,
  "raw_response": "```json\n{\n  \"page_number\": 4,\n  \"markdown\": \"![Figure 2: Diagram comparing three types of vision encoders in popular VLMs: Vary/DeepSeekVL (Dual-tower), InternVL series (Tile-based), and Qwen2/3VL series (Adaptive resolution). It lists deficiencies for each, such as unsupported pipeline parallel, too many vision tokens, and large activations.]\\n\\n**Figure 2 | Typical vision encoders in popular VLMs.** Here are three types of encoders commonly used in current open-source VLMs, all of which suffer from their respective deficiencies.\\n\\n# 2. Related Works\\n\\n## 2.1. Typical Vision Encoders in VLMs\\n\\nCurrent open-source VLMs employ three main types of vision encoders, as illustrated in Figure 2. The first type is a dual-tower architecture represented by Vary [36], which utilizes parallel SAM [17] encoder to increase visual vocabulary parameters for high-resolution image processing. While offering controllable parameters and activation memory, this approach suffers from significant drawbacks:"
}