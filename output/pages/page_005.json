{
  "page_number": 5,
  "raw_response": "```json\n{\n  \"page_number\": 5,\n  \"markdown\": \"![Figure 3: The architecture of DeepSeek-OCR.](figure_3_diagram)\\n\\n**Figure 3 | The architecture of DeepSeek-OCR.** DeepSeek-OCR consists of a DeepEncoder and a DeepSeek-3B-MoE decoder. DeepEncoder is the core of DeepSeek-OCR, comprising three components: a SAM [17] for perception dominated by window attention, a CLIP [29] for knowledge with dense global attention, and a 16× token compressor that bridges between them.\\n\\n## 3. Methodology\\n\\n### 3.1. Architecture\\n\\nAs shown in Figure 3, DeepSeek-OCR enjoys a unified end-to-end VLM architecture consisting of an encoder and a decoder. The encoder (namely DeepEncoder) is responsible for extracting image features and tokenizing as well as compressing visual representations. The decoder is used for generating the required result based on image tokens and prompts. DeepEncoder is approximately 380M in parameters, mainly composed of an 80M SAM-base [17] and a 300M CLIP-large [29] connected in series. The decoder adopts a 3B MoE [19, 20] architecture with 570M activated parameters. In the following paragraphs, we will delve into the model components, data engineering, and training skills.\\n\\n### 3.2. DeepEncoder\\n\\nTo explore the feasibility of contexts optical compression, we need a vision encoder with the following features: 1.Capable of processing high resolutions; 2.Low activation at high resolutions; 3.Few vision tokens; 4.Support for multiple resolution inputs; 5. Moderate parameter count. However, as described in the Section 2.1, current open-source encoders cannot fully satisfy all these conditions. Therefore, we design a novel vision encoder ourselves, named DeepEncoder.\\n\\n#### 3.2.1. Architecture of DeepEncoder\\n\\nDeepEncoder mainly consists of two components: a visual perception feature extraction component dominated by window attention, and a visual knowledge feature extraction component with dense global attention. To benefit from the pretraining gains of previous works, we use SAM-base (patch-size 16) and CLIP-large as the main architectures for the two components respectively. For CLIP, we remove the first patch embedding layer since its input is no longer images but output tokens from the previous pipeline. Between the two components, we borrow from Vary [36] and use a 2-layer convolutional module to perform 16× downsampling of vision tokens. Each convolutional layer has a kernel size of 3, stride of 2, padding of 1, and channels increase from 256 to 1024. Assuming we input a 1024×1024 image, the DeepEncoder will segment it into 1024/16×1024/16=4096 patch tokens. Since the first half of encoder is dominated by window attention and only 80M, the activation is acceptable. Before entering global attention,\",\n  \"entities\": [\n    \"DeepSeek-OCR\",\n    \"DeepEncoder\",\n    \"DeepSeek-3B-MoE\",\n    \"SAM\",\n    \"CLIP\",\n    \"VLM\",\n    \"SAM-base\",\n    \"CLIP-large\",\n    \"Vary\",\n    \"ViTDet\",\n    \"ViT\",\n    \"MoE\"\n  ],\n  \"summary\": \"This page details the methodology and architecture of DeepSeek-OCR, specifically focusing on its unified end-to-end VLM structure. The system comprises a custom vision encoder called DeepEncoder and a DeepSeek-3B-MoE decoder. The DeepEncoder is designed to handle high resolutions efficiently by combining an 80M SAM-base model (local attention) and a 300M"
}