{
  "page_number": 8,
  "raw_response": "```json\n{\n  \"page_number\": 8,\n  \"markdown\": \"![Figure 5: Comparison of (a) Ground truth image containing geometry and text, and (b) Fine annotations with layouts showing XML-like tags with coordinates.](figure_5)\\n\\n**Figure 5 | OCR 1.0 fine annotations display.** We format the ground truth into an interleaved layout and text format, where each paragraph of text is preceded by the coordinates and label of it in the original image. All coordinates are normalized into 1000 bins.\\n\\ndirectly from the full dataset using *fitz*, aimed at teaching the model to recognize optical text, especially in minority languages. Fine annotations include 2M pages each for Chinese and English, labeled using advanced layout models (such as PP-DocLayout [33]) and OCR models (such as MinuerU [34] and GOT-OCR2.0 [38]) to construct detection and recognition interleaved data. For minority languages, in the detection part, we find that the layout model enjoys certain generalization capabilities. In the recognition part, we use *fitz* to create small patch data to train a GOT-OCR2.0, then use the trained model to label small patches after layout processing, employing a model flywheel to create 600K data samples. During the training of DeepSeek-OCR, coarse labels and fine labels are distinguished using different prompts. The ground truth for fine annotation image-text pairs can be seen in Figure 5. We also collect 3M *Word* data, constructing high-quality image-text pairs without layout by directly extracting content. This data mainly brings benefits to formulas and HTML-formatted tables. Additionally, we select some open-source data [28, 37] as supplements.\\n\\nFor natural scene OCR, our model mainly supports Chinese and English. The image data sources come from LAION [31] and Wukong [13], labeled using PaddleOCR [9], with 10M data samples each for Chinese and English. Like document OCR, natural scene OCR can also control whether to output detection boxes through prompts.\\n\\n### 3.4.2. OCR 2.0 data\\n\\nFollowing GOT-OCR2.0 [38], we refer to chart, chemical formula, and plane geometry parsing data as OCR 2.0 data. For chart data, following OneChart [7], we use pyecharts and matplotlib\",\n  \"entities\": [\n    \"OCR 1.0\",\n    \"OCR 2.0\",\n    \"fitz\",\n    \"PP-DocLayout\",\n    \"MinuerU\",\n    \"GOT-OCR2.0\",\n    \"DeepSeek-OCR\",\n    \"Word data\",\n    \"LAION\",\n    \"Wukong\",\n    \"PaddleOCR\",\n    \"OneChart\",\n    \"pyecharts\",\n    \"matplotlib\",\n    \"Chinese\",\n    \"English\"\n  ],\n  \"summary\": \"This page details the data preparation process for an OCR model, specifically distinguishing between 'OCR 1.0 fine annotations' and 'OCR 2"
}