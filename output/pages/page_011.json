{
  "page_number": 11,
  "raw_response": "```json\n{\n  \"page_number\": 11,\n  \"markdown\": \"Table 3 | We use OmniDocBench [27] to test the performance of DeepSeek-OCR on real document parsing tasks. All metrics in the table are edit distances, where smaller values indicate better performance. \\\"Tokens\\\" represents the average number of vision tokens used per page, and \\\"$^{\\\\dagger 200dpi}$\\\" means using *fitz* to interpolate the original image to 200dpi. For the DeepSeek-OCR model, the values in parentheses in the \\\"Tokens\\\" column represent valid vision tokens, calculated according to Equation 1.\\n\\n| Model | Tokens | English overall | English text | English formula | English table | English order | Chinese overall | Chinese text | Chinese formula | Chinese table | Chinese order |\\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\\n| **Pipeline Models** | | | | | | | | | | | |\\n| Dolphin [11] | - | 0.356 | 0.352 | 0.465 | 0.258 | 0.35 | 0.44 | 0.44 | 0.604 | 0.367 | 0.351 |\\n| Marker [1] | - | 0.2"
}